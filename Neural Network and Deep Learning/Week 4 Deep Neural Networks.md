# 1. 深度神经网络

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572770080819.png" alt="1572770080819" style="zoom:80%;" />
    <br>
    <b>双隐层神经网络</b>
</div>

​		把隐层的层数当作超参数，在保留交叉验证数据上评估

| 符号                      | 含义                            |
| ------------------------- | ------------------------------- |
| $L$                       | 层数（注意不包括输入层）        |
| $n^{[l]}, l=1,2,\cdots,L$ | 第$l$层的神经元数目             |
| $a^{[l]}=g(z^{[l]})$      | 第$i$层的激活函数               |
| $W^{[l]}, b^{[l]}$        | 第$i$层的权重、偏置             |
| $x$                       | 输入特征（同时也是第0层的权重） |

# 2. 深层网络中的前向传播

​		要仔细计算矩阵操作的维度

# 3. 核对矩阵的维数

​		一般先忽略掉$b^{[l]}$，以便观察其他矩阵的维数

| 变量                                                         | 非向量化的矩阵维数     | 向量化的矩阵维数       |
| ------------------------------------------------------------ | ---------------------- | ---------------------- |
| $dW^{[l]},\ W^{[l]}$                                         | $(n^{[l]}, n^{[l-1]})$ | $(n^{[l]}, n^{[l-1]})$ |
| $x$（或$X$）                                                 | $(n^{[0]},1)$          | $(n^{[0]},m)$          |
| $db^{[l]},\ b^{[l]}$                                         | $(n^{[l]}, 1)$         | $(n^{[l]}, 1)$         |
| $dz^{[l]},\ z^{[l]},\ a^{[l]}$（或$dZ^{[l]},\ Z^{[l]},\ A^{[l]}$） | $(n^{[l]}, 1)$         | $(n^{[l]}, m)$         |

# 4. 为什么使用深层表示

​		神经网络前几层类似特征（如边缘）检测器，再结合后几层，就可以学习复杂的函数。即寻找小范围的简单特征（如边缘），再一步步到更大更复杂的特征。

​		根据电路理论，<u>许多函数可以用较深且较小（隐藏层的单元少）的网络来计算，但若网络变浅，则需要指数级增大（即隐藏层单元数指数级增大）的网络来计算。</u>例如：

​		对于函数$y=x_1\oplus x_2\oplus \cdots\oplus x_n$：

​		通过较深且较小的网络计算$y$（网络深度：$\mathcal{O}(\log(n))$，各层单元个数：$n^{[l]}=2^{\lceil\log(n)\rceil-l}$）：

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572775720357.png" alt="1572775720357" style="zoom:80%;" />
</div>

​		通过单隐层但较大的网络计算$y$（网络深度：$1$，隐层单元个数：$2^n$）：

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572776994493.png" alt="1572776994493" style="zoom:80%;" />
</div>



​		这是因为单隐层需要穷举$2^n$种输入以找出正确的值。

# 5. 搭建深层神经网络块

​		一般需要将$z^{[l]}$缓存起来以便后面的使用。

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572778167371.png" alt="1572778167371" style="zoom:80%;" />
    <br>
    <b>第l层的计算过程</b>
</div>

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572778990751.png" alt="1572778990751" style="zoom:80%;" />
    <br>
    <b>神经网络的一次梯度下降循环</b>
</div>



# 6. 前向和反向传播

- 第$l$层的前向传播：

| 前向传播 | 非向量化                                                     | 向量化                                                       |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 输入     | $a^{[l-1]}$                                                  | $A^{[l-1]}$                                                  |
| 输出     | $a^{[l]}$和缓存（$z^{[l]}$，也可缓存$W^{[l]},\ b^{[l]}$）    | $A^{[l]}$和缓存（$Z^{[l]}$）                                 |
| 计算过程 | $\begin{aligned}z^{[l]}&=W^{[l]}a^{[l-1]}+b^{[l]}\\a^{[l]}&=g^{[l]}(z^{[l]})\end{aligned}$ | $\begin{aligned}Z^{[l]}&=W^{[l]}A^{[l-1]}+b^{[l]}\\A^{[l]}&=g^{[l]}(Z^{[l]})\end{aligned}$ |

- 第$l$层的反向传播：

| 反向传播 | 非向量化                                                     | 向量化                                                       |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 输入     | $da^{[l]}$                                                   | $A^{[l-1]}$                                                  |
| 输出     | $da^{[l-1]},\ dW^{[l]},\ db^{[l]}$                           | $dA^{[l-1]},\ dW^{[l]},\ db^{[l]}$                           |
| 计算过程 | $$\begin{aligned}dz^{[l]}&=da^{[l]}*g^{[l]}(z^{[l]})\\dW^{[l]}&=dz^{[l]}a^{[l-1]}\\db^{[l]}&=dz^{[l]}\\da^{[l-1]}&=W^{[l]T}dz^{[l]}\end{aligned}$$ | $$\begin{aligned}dZ^{[l]}&=dA^{[l]}*g^{[l]'}(Z^{[l]})\\dW^{[l]}&=\frac{1}{m}dZ^{[l]}A^{[l-1]T}\\db^{[l]}&=\frac{1}{m}np.sum(dZ^{[l]},\ axis=1,\ keepdims=True)\\dA^{[l-1]}&=W^{[l]T}dZ^{[l]}\end{aligned}$$ |

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572786371806.png" alt="1572786371806" style="zoom:80%;" />
    <br>
    <b>神经网络计算简图</b>
</div>

​		在二分分类中，$da^{[l]}=-\frac{y}{a}+\frac{1-y}{1-a}$。

​		深度学习的应用是一个经验性的过程

# 7. 参数 VS 超参数

- 参数：$W^{[l]}, b{[l]}, l=1,2,\cdots,L$

- 超参数：

  - 学习速率$\alpha$

  - 迭代次数

  - 层数$L$

  - 隐层单元数$n^{[l]}, l=1,2,\cdots,L-1$

  - 激活函数的选择

  - （momentum, mini batch的大小，正则化参数等等

    的

<div align="center">
    <img src="Week 4 Deep Neural Networks.assets/1572787911071.png" alt="1572787911071" style="zoom:80%;" />
</div>

​		深度学习的应用是一个经验性的过程。例如通过改变$\alpha$来观察损失$J$的变化，以选取合适的$\alpha$。

​		数据、运行环境等都会变化，因此要经常尝试不同的超参数取值，以训练更好的超参数直觉。

# 8. 这和大脑有什么关系？

​		关系不大。

