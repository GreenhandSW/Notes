# 1. 神经网络概览
| 上标  | 含义            |
| :---: | --------------- |
| $[i]$ | 第$i$层         |
| $(i)$ | 第$i$个训练样本 |
**正向传播**：正向计算$\displaystyle z^{[i]}, a^{[i]}, \mathcal{L}(a^{[i]}, y)$。
**反向传播**：反向计算$\displaystyle dw, db$等。

# 2. 神经网络表示
![双层（单隐层）神经网络](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571243678015&di=009836ec880c450d05e88c5faf56ea73&imgtype=0&src=http%3A%2F%2Fimg2018.cnblogs.com%2Fblog%2F1499410%2F201909%2F1499410-20190910202204204-238095909.png)
<center><strong>双层（单隐层）神经网络</strong></center>

输入层$\to$隐藏层$\to$输出层
输入特征的数值也可用$a^{[0]}$表示，即对第$0$层有$x=a^{[0]}$，其中$a$表示“**激活**”，表示网络中不同层的值会传递给后面的层。$a^{[j]}_{i}$表示第$j$层的第$i$个单元。
输出层（此图中为$a^{[2]}$）的值即预测值$\hat{y}$。

# 3. 计算神经网络的输出
| 非向量化 | $z^{[j]}_{i}=w^{[j]T}_{i}x+b^{[j]}_{i}, a^{[j]}_{i}=\sigma(z^{[j]}_{i})$ |
| -------- | :----------------------------------------------------------: |
| 向量化   |     $z^{[j]}=W^{[j]}x+b^{[j]}, a^{[j]}=\sigma(z^{[j]})$      |

​		其中：
$$
W^{[ j]} =W^{T}=\begin{bmatrix}
w^{[ j]}_{1}\\
w^{[ j]}_{2}\\
\vdots \\
w^{[ j]}_{m}
\end{bmatrix}^{T}
$$


注：

>由于第$j$层的输入实际上是第$j-1$层的输出，因此，
>对第1层有：
>$z^{[1]}=W^{[1]}x+b^{[1]}$，或$z^{[1]}=W^{[1]}a^{[0]}+b^{[1]}$
>$a^{[1]}=\sigma(z^{[1]})$
>但对第二层有：
>$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
>$a^{[2]}=\sigma(z^{[2]})$
>对最后一层有$\hat{y}=a^{[j]}$


